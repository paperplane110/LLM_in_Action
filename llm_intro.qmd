# LLM 的前世今生

2022 年 11 月 30 日，OpenAI 正式发布了其面向消费用户的产品——ChatGPT。ChatGPT 一经发布便激起了圈内、圈外的广泛讨论——毕竟已经很长时间没有一种类似的技术可以引起如此广泛的讨论，ChatGPT 的发布也标志着大语言模型（LLM: Large Language Model） 时代的到来。看起来，刚刚要崛起的`元宇宙`，在这股风潮之下，也失去了往日的喧嚣。

根据 [热搜引擎](https://weibo.zhaoyizhe.com/superInfo.html?topic=ChatGPT) 提供的微博热搜历史数据，我们发现，2022 年 12 月 5 日，ChatGPT 第一次登上微博热搜榜，其最后的在榜时间为 2023 年 3 月 31 日，累计在榜时长达到了 1391 分钟。

![ChatGPT登上微博热搜](./images/llm_chatgpt_wb_hs.jpg){#fig-llm_chatgpt_wb_hs}

根据 [百科星图](https://baike.baidu.com/starmap/view?nodeId=e0a309bf5b0f017891c7b859) 可知，目前谷歌、亚马逊、百度、阿里等多个科技巨头都加入到了 *对话式大语言模型* 的研发中。

* 2023 年 2 月 6 日，谷歌宣布将推出一款聊天机器人——Bard，2 月 9 日，谷歌 Bard 发布会试演翻车，回答内容出现错误，当日市值暴跌1000亿美元。
* 2023 年 2 月 24 日，Meta 官宣 SOTA 大语言模型 LLaMA，对非商用的研究用例开源。
* 2023 年 3 月 14 日，斯坦福发布了一个由 LLaMA 7B 微调的模型 Alpaca，性能和 GPT-3.5 不相上下。
* 2023 年 3 月 14 日，OpenAI 发布 GPT-4。
* 2023 年 3 月 16 日，百度举办“百度文心一言新闻发布会”，正式发布 *文心一言*。
* 2023 年 4 月 11 日，阿里在阿里云峰会上，正式宣布推出大语言模型 *通义千问*。
* 2023 年 7 月 18 日，Meta 官宣发布 LLaMA2。
* 2023 年 10 月 17 日，百度在 2023 年的百度世界大会上宣布发布 *文心 4.0*。
* ……

根据 [-@zhao2023survey]，在学术界，ChatGPT 发布之后，和大模型相关的论文的数量也呈现出爆发式增长。

::: {#fig-trend_for_llm layout-ncol=2}

![Query = "Language Model"](./images/treand_lm.jpg){#fig-trend_lm}

![Query = "Large Language Model"](./images/trend_llm.jpg){#fig-trend_llm}

[arXiv](https://arxiv.org/) 论文库中“大语言模型”的论文数量趋势图
:::

## 大语言模型族谱
[-@vaswani2023attention] 可以称之为大语言模型的鼻祖和源泉，在 [-@vaswani2023attention] 中，谷歌机器翻译团队提出了由多组 Encoder/Decoder 构成的机器翻译模型 Transformer，而 Transformer 模型也成为了一切的起点。之后，大模型的发展大致走上了两条路：

* 一条路是舍弃 Decoder 部分，仅仅使用 Encoder 部分的自编码语言模型[^1]，其最出名的代表就是 Bert 家族。
* 一条路是舍弃 Encoder 部分，仅仅基于 Decoder 部分的自回归语言模型[^2]，而 ChatGPT 背后的 GPT[^3] 家族则属于 Decoder-only 的分支。

[-@yang2023harnessing] 给出了如 @fig-llm_family_tree 所示的大语言模型的族谱。

![大语言模型族谱](./images/LLMTree.jpeg){#fig-llm_family_tree}

在大语言模型发展的早期，以 Bert 为代表的自编码模型突飞猛进，但是由于没有突破 Scale Law [^4] 法则，因此其发展速度也凋零。反之，由于 GPT 的研究人员发现：扩大语言模型的规模可以显著提高零样本与小样本的学习的能力——也即突破了 Scale Law，以 GPT 为代表的自回归分支则更加枝繁叶茂，成为了当下大模型发展的主流分支。

## GPT 的贡献
[-@zhao2023survey] 对 LLM 的相关能力和 GPT 的相关进展做了详细的描述，这里我们重点说一下 GPT 对 LLM 发展的核心贡献。

### 预训练+微调的模型架构
2018 年，OpenAI 发表了论文 [-@radford2018improving]，这就是 GPT-1。GPT-1 提出的预训练+微调的方法可以更好的利用大量的预训练数据，从而让模型能够更好的适应各种特定任务。虽然当时还存在一些局限性，例如当时还不能根据一个给定的标题来生成一篇新闻报道，但是 GPT-1 所开创的这种 预训练+微调 的模型架构，对 NLP 的后续发展具有深远的影响。

* 在预训练阶段，模型会在大规模无标注文本上进行无监督学习，提取通用的语言特征。
* 在微调阶段，模型会在特定任务上进行有监督的学习，以适应不同的任务需求。

### 迁移学习能力
为了解决 GPT-1 的问题，2019 年，OpenAI 发布了 GPT-2，论文 [-@Radford2019LanguageMA] 对 GPT-2 进行了详细的阐述。通过增加模型参数和数据量，GPT-2 极大的提高了模型的泛化能力和生成能力。除了在特定任务上表现较好（例如更具标题生成文章）之外，GPT-2 还初步表现出一定的零样本或少量样本学习能力。这使得 GPT-2 能够适用于多种自然语言处理任务，例如：翻译，问答，摘要生成，文本生成等，而在 GPT-2 之前，这些特殊任务需要设计专门的模型来分别实现。GPT-2 通过实践证明通过海量数据和大量参数训练出来的词向量模型在不经过额外的特殊训练下就可以迁移到不同类别的任务。

GPT-2 最大的贡献也在于他通过实践验证了大模型的迁移学习能力。

### 上下文学习能力和涌现
2020 年，OpenAI 发布了 1750 亿参数规模的、性能更加强大的 GPT-3。[-@NEURIPS2020_1457c0d6] 中提到，GPT-3 提出了上下文学习（ICL：in-context learning）的概念。ICL 可以指导 LLM 理解以自然语言形式呈现的任务，利用 ICL 的能力，我们可以通过优化给 LLM 的输入以获取更好的结果。在 ICL 的加持下，@sec-prompt_engineering 中介绍的提示词工程才得以成为可能。

GPT-3 在多种自然语言处理任务上展现出了惊人的性能，甚至可以仅通过简单的提示词来适应不同的处理任务。而研究人员实际上没有在 GPT-3 训练完之前就预测到模型有这么强大的能力，GPT-3 的实践证明，LLM 可以具备涌现能力（Emergent Ability）。

### 代码能力和指令遵循能力
为了进一步提升模型的性能，OpenAI 继续探索了两种主要方法：基于代码数据训练和，与人类偏好保持一致。

2021 年，OpenAI 在 [-@chen2021evaluating] 中推出了在大量 GitHub 代码语料库上微调的 GPT 模型——Codex。Codex 可以解决非常复杂的编程问题，并且还可以显著提高解决数学问题的性能。目前，大名鼎鼎的 [Github Copilot](https://github.com/features/copilot) 就是基于 [Codex](https://openai.com/blog/openai-codex) 模型而研发。

2022 年，OpenAI 在 [-@ouyang2022training] 中推出了基于 RLHF 技术的增强版 GPT-3——InstructGPT。InstructGPT 在指令遵循方面对 GPT-3 模型做了微调，使得其更善于遵循用户的意图。

代码能力和指令遵循能力进一步增强了 GPT-3 模型的能力，OpenAI 将其称之为 GPT-3.5。而 ChatGPT 刚刚推出的时候，其背后默认的模型就是 GPT-3.5。

所以，从整个的 GPT 的历程看，从 2018 年 ~ 2022 年，在长达 5 年多的时间里，OpenAI 一步一步通过探索和实践，让大模型应该具备的相关能力一点一点的浮出水面，进入我们的视野。

[^1]: 自编码模型：自编码语言模型通过随机Mask输入的部分单词，然后预训练的目标是预测被Mask的单词，不仅可以融入上文信息，还可以自然的融入下文信息。
[^2]: 自回归模型：自回归语言模型根据输入序列中的前面的内容来预测序列中的下一个词。自回归模型只能利用上文或者下文的信息，不能同时利用上文和下文的信息。
[^3]: GPT，Generative Pre-trained Transformer，基于 Transformer 的生成式预训练模型。
[^4]: Scaling Laws：随着模型大小、数据集大小和用于训练的计算浮点数的增加，模型的性能会提高。为了获得模型的最佳性能，所有三个因素必须同时放大。当不受其他两个因素的制约时，模型性能与每个单独的因素都有幂律关系。