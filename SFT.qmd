# 微调 {#sec-sft}
对于 ChatGPT 或者 文心 这些大模型而言，对其进行预训练的成本非常大，整个训练需要数千个 GPU 并行处理才可以解决海量数据的算力问题。而仅 GPU 的成本就可能就高达数百万美元。根据 OpenAI 的 GPT-3 技术概述，每次训练至少需要价值 500 万美元的 GPU。在 7 月份的麻省理工学院活动中，当被问及训练基础模型的成本是否在 5000 万至 1 亿美元之间时，OpenAI 的联合创始人 Sam Altman 回答说，这“不止于此”，而且越来越贵。据估计，2023 年 1 月，ChatGPT 使用了近 30000 个 GPU 来处理数亿的日常用户请求。华盛顿大学电气和计算机工程助理教授 Sajjad Moazeni 表示，这些查询每天可能消耗约 1 GWh，相当于约 3.3 万个美国家庭的日常能源消耗。[^1]

基于对成本的考虑，因此，预训练一个大模型并非是人人都可以做到的事情。

微调（SFT：Supervised Fine Tuning）是一种机器学习技术，涉及对预训练模型进行小的调整，以提高其在特定任务中的性能。因为模型已经对世界有了很好的了解，并且可以利用这些知识更快地学习新任务，因此微调比从头开始训练模型更有效，通常也会产生更好的结果。从成本的角度看，微调的成本也会比预训练大模型的成本要低的多。虽然微调也需要成本，但我们基本可以承担这些成本。

## 为什么微调
除了成本因素之外，微调在机器学习中具有如此重要意义的原因还包括：

* 数据效率：微调允许使用有限的特定任务数据进行有效的模型自适应。可以使用现有的预先训练模型，并根据任务对其进行优化，而不是收集和标注新的数据集。因此，从数据处理效率层面而言，微调会节省更多的时间和资源。
* 时间效率：从头开始训练模型需要很长时间，而因为微调是从模型已经学习的特征开始，因此减少了收敛所需的时间，进而加快了训练的过程，提升了训练的效率。
* 知识迁移：预训练模型在最初的训练中已经从大量数据集中学习到了有价值的特征和模式。微调可以将所获得的知识转移到特定任务中，微调可以使预训练模型在特定任务上有一种增强的效果。
* 专业化：微调可以允许我们自定义一个模型，使其在特定任务中表现更加出色。通过调整模型的设置，可以创建一个在特定情况下非常有效的工具。

## 何时微调
虽然微调的成本比从头预训练大模型的成本要小的多的多，但是对模型进行微调仍然需要我们投入时间和精力。微调不是没有成本，只是和预训练大模型相比成本小而已。

因此，在准备微调之前，我们最好先尝试通过提示工程（@sec-prompt_engineering）、RAG（@sec-RAG）、或通过类似 Agent （@sec-agent）的函数调用来获得更好的结果。

:::{.callout-note}
并非所有的大模型都支持函数调用，目前 OpenAI 的 GPT 是支持函数调用的，所以如果使用 GPT，则可以直接使用函数调用 API 来实现函数调用。

对于不具备函数调用能力的大模型，可以考虑通过 Agent 的方式（例如使用 （@sec-lc_react）的 LangChain Agent 能力）来调用外部工具或函数。
:::

在准备微调之前，我们需要进行仔细的分析和考虑：

* 模型在许多任务上可能最初表现不佳，但使用正确的提示词可以改善结果，此时可能不需要微调。
* 迭代微调需要创建数据集并运行训练任务，因此迭代提示词比迭代微调快得多。

## 如何微调
不同平台的微调方式各不相同，所以微调的方式需要参考具体使用的平台：

* 百度文心大模型微调方式：[SFT 文档](https://cloud.baidu.com/doc/WENXINWORKSHOP/s/qlgujhcpo)
* GPT 大模型微调方式：[Fine-tuning 文档](https://platform.openai.com/docs/guides/fine-tuning)


[^1]: [What Large Models Cost You – There Is No Free AI Lunch](https://www.forbes.com/sites/craigsmith/2023/09/08/what-large-models-cost-you--there-is-no-free-ai-lunch/)